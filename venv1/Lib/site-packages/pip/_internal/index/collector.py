"""
Module responsible for collecting package links from configured index
and find-links locations.
"""

from __future__ import annotations

import collections
import email.message
import functools
import itertools
import json
import logging
import os
import urllib.parse
import urllib.request
from collections.abc import Iterable, MutableMapping, Sequence
from dataclasses import dataclass
from html.parser import HTMLParser
from optparse import Values
from typing import Callable, NamedTuple, Protocol

from pip._vendor import requests
from pip._vendor.requests import Response
from pip._vendor.requests.exceptions import RetryError, SSLError

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.models.link import Link
from pip._internal.models.search_scope import SearchScope
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status
from pip._internal.utils.filetypes import is_archive_file
from pip._internal.utils.misc import redact_auth_from_url
from pip._internal.vcs import vcs

from .sources import CandidatesFromPage, LinkSource, build_source

logger = logging.getLogger(__name__)

ResponseHeaders = MutableMapping[str, str]


# --------------------------------------------------------------------
# Helpers
# --------------------------------------------------------------------

def _match_vcs_scheme(url: str) -> str | None:
    for scheme in vcs.schemes:
        if url.lower().startswith(scheme) and url[len(scheme)] in "+:":
            return scheme
    return None


class _NotAPIContent(Exception):
    def __init__(self, content_type: str, request_desc: str) -> None:
        super().__init__(content_type, request_desc)
        self.content_type = content_type
        self.request_desc = request_desc


class _NotHTTP(Exception):
    pass


def _ensure_api_header(response: Response) -> None:
    content_type = response.headers.get("Content-Type", "Unknown").lower()
    allowed = (
        "text/html",
        "application/vnd.pypi.simple.v1+html",
        "application/vnd.pypi.simple.v1+json",
    )
    if not content_type.startswith(allowed):
        raise _NotAPIContent(content_type, response.request.method)


def _ensure_api_response(url: str, session: PipSession) -> None:
    scheme = urllib.parse.urlsplit(url).scheme
    if scheme not in {"http", "https"}:
        raise _NotHTTP()

    resp = session.head(url, allow_redirects=True)
    raise_for_status(resp)
    _ensure_api_header(resp)


def _get_simple_response(url: str, session: PipSession) -> Response:
    if is_archive_file(Link(url).filename):
        _ensure_api_response(url, session)

    logger.debug("Fetching %s", redact_auth_from_url(url))

    resp = session.get(
        url,
        headers={
            "Accept": ", ".join(
                [
                    "application/vnd.pypi.simple.v1+json",
                    "application/vnd.pypi.simple.v1+html; q=0.1",
                    "text/html; q=0.01",
                ]
            ),
            "Cache-Control": "max-age=0",
        },
    )
    raise_for_status(resp)
    _ensure_api_header(resp)

    return resp


def _get_encoding_from_headers(headers: ResponseHeaders) -> str | None:
    if headers and "Content-Type" in headers:
        msg = email.message.Message()
        msg["content-type"] = headers["Content-Type"]
        return msg.get_param("charset")
    return None


# --------------------------------------------------------------------
# Index content + parsing
# --------------------------------------------------------------------

@dataclass(frozen=True)
class IndexContent:
    content: bytes
    content_type: str
    encoding: str | None
    url: str
    cache_link_parsing: bool = True

    def __str__(self) -> str:
        return redact_auth_from_url(self.url)


class HTMLLinkParser(HTMLParser):
    def __init__(self, url: str) -> None:
        super().__init__(convert_charrefs=True)
        self.url = url
        self.base_url: str | None = None
        self.anchors: list[dict[str, str | None]] = []

    def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:
        if tag == "base" and self.base_url is None:
            for name, value in attrs:
                if name == "href":
                    self.base_url = value
        elif tag == "a":
            self.anchors.append(dict(attrs))


class ParseLinks(Protocol):
    def __call__(self, page: IndexContent) -> Iterable[Link]: ...


def with_cached_index_content(fn: ParseLinks) -> ParseLinks:
    @functools.cache
    def wrapper(page: IndexContent) -> list[Link]:
        return list(fn(page))

    def inner(page: IndexContent) -> list[Link]:
        if page.cache_link_parsing:
            return wrapper(page)
        return list(fn(page))

    return inner


@with_cached_index_content
def parse_links(page: IndexContent) -> Iterable[Link]:
    content_type = page.content_type.lower()

    if content_type.startswith("application/vnd.pypi.simple.v1+json"):
        data = json.loads(page.content)
        for file in data.get("files", []):
            link = Link.from_json(file, page.url)
            if link:
                yield link
        return

    parser = HTMLLinkParser(page.url)
    encoding = page.encoding or "utf-8"
    parser.feed(page.content.decode(encoding))

    base_url = parser.base_url or page.url
    for anchor in parser.anchors:
        link = Link.from_element(anchor, page_url=page.url, base_url=base_url)
        if link:
            yield link


# --------------------------------------------------------------------
# Link Collector
# --------------------------------------------------------------------

class CollectedSources(NamedTuple):
    find_links: Sequence[LinkSource | None]
    index_urls: Sequence[LinkSource | None]


class LinkCollector:
    def __init__(self, session: PipSession, search_scope: SearchScope) -> None:
        self.session = session
        self.search_scope = search_scope

    @classmethod
    def create(
        cls,
        session: PipSession,
        options: Values,
        suppress_no_index: bool = False,
    ) -> "LinkCollector":
        index_urls = [options.index_url] + options.extra_index_urls

        if options.no_index and not suppress_no_index:
            index_urls = []

        search_scope = SearchScope.create(
            find_links=options.find_links or [],
            index_urls=index_urls,
            no_index=options.no_index,
        )

        return cls(session=session, search_scope=search_scope)

    def fetch_response(self, location: Link) -> IndexContent | None:
        try:
            resp = _get_simple_response(location.url, self.session)
        except Exception as exc:
            logger.debug("Failed to fetch %s: %s", location, exc)
            return None

        return IndexContent(
            content=resp.content,
            content_type=resp.headers.get("Content-Type", ""),
            encoding=_get_encoding_from_headers(resp.headers),
            url=resp.url,
            cache_link_parsing=location.cache_link_parsing,
        )

    def collect_sources(
        self,
        project_name: str,
        candidates_from_page: CandidatesFromPage,
    ) -> CollectedSources:

        index_sources = collections.OrderedDict(
            build_source(
                loc,
                candidates_from_page=candidates_from_page,
                page_validator=self.session.is_secure_origin,
                expand_dir=False,
                cache_link_parsing=False,
                project_name=project_name,
            )
            for loc in self.search_scope.get_index_urls_locations(project_name)
        ).values()

        find_sources = collections.OrderedDict(
            build_source(
                loc,
                candidates_from_page=candidates_from_page,
                page_validator=self.session.is_secure_origin,
                expand_dir=True,
                cache_link_parsing=True,
                project_name=project_name,
            )
            for loc in self.search_scope.find_links
        ).values()

        return CollectedSources(
            find_links=list(find_sources),
            index_urls=list(index_sources),
        )